---
abstract: "Unsupervised image-to-image translation (UNIT) aims at learning a
  mapping between several visual domains by using unpaired training images.
  Recent studies have shown remarkable success for multiple domains but they
  suffer from two main limitations: they are either built from several
  two-domain mappings that are required to be learned independently, or they
  generate low-diversity results, a problem known as mode collapse. To overcome
  these limitations, we propose a method named GMM-UNIT, which is based on a
  content-attribute disentangled representation where the attribute space is
  fitted with a GMM. Each GMM component represents a domain, and this simple
  assumption has two prominent advantages. First, it can be easily extended to
  most multi-domain and multi-modal image-to-image translation tasks. Second,
  the continuous domain encoding allows for interpolation between domains and
  for extrapolation to unseen domains and translations. Additionally, we show
  how GMM-UNIT can be constrained down to different methods in the literature,
  meaning that GMM-UNIT is a unifying framework for unsupervised image-to-image
  translation. "
draft: false
url_pdf: https://arxiv.org/abs/2003.06788
title: "GMM-UNIT: Unsupervised Multi-Domain and Multi-Modal Image-to-Image
  Translation via Attribute Gaussian Mixture Modeling"
publication_types:
  - "3"
authors:
  - Y. Liu
  - M. De Nadai
  - J. Yao
  - N. Sebe
  - B. Lepri
  - X. Alameda-Pineda
publication: Preprint
featured: false
categories:
  - Computer vision
image:
  filename: featured
  focal_point: Smart
  preview_only: false
date: 2020-03-15T11:28:52.129Z
url_code: https://github.com/yhlleo/GMM-UNIT
---
